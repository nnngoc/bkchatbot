{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-ai-ml --quiet\n",
    "%pip install azure-identity --quiet\n",
    "%pip install mlflow --quiet\n",
    "%pip install azureml-mlflow --quiet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mport tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "import mlflow.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import os\n",
    "import json\n",
    "\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import (\n",
    "    ManagedOnlineEndpoint,\n",
    "    ManagedOnlineDeployment,\n",
    "    Model,\n",
    "    Environment,\n",
    "    CodeConfiguration,\n",
    ")\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.deployments import get_deploy_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['AZURE_CLIENT_ID'] = \"4fbdc131-206d-4b9f-b1e4-0cdae69b7882\"\n",
    "os.environ['AZURE_TENANT_ID'] = \"52bfa1a4-23cf-44cd-ab26-8f4718d7b686\"\n",
    "os.environ['AZURE_CLIENT_SECRET'] = \"ZcA8Q~X2cQgfWAhTzi_glMtZvX4etae_DX1rMbLe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter details of your Azure Machine Learning workspace\n",
    "subscription_id = \"6a3037c7-8da4-4476-b0a8-cd53742992b5\"\n",
    "resource_group = \"bkheart\"\n",
    "workspace = \"bkheart\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    DefaultAzureCredential(), subscription_id, resource_group, workspace\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azureml_tracking_uri = ml_client.workspaces.get(\n",
    "    ml_client.workspace_name\n",
    ").mlflow_tracking_uri\n",
    "mlflow.set_tracking_uri(azureml_tracking_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.get_tracking_uri()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow import Tensor\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "from tensorflow.keras import Sequential, Input, Model\n",
    "from tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D, Dropout, Bidirectional, LSTM, TimeDistributed\n",
    "from tensorflow_addons.text import crf_log_likelihood\n",
    "from tensorflow_addons.layers import CRF\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.utils import class_weight as sklearn_class_weight\n",
    "from transformers import PhobertTokenizer, TFAutoModel\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install py_vncorenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import py_vncorenlp\n",
    "py_vncorenlp.download_model()\n",
    "from vncorenlp import VnCoreNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Loader:\n",
    "    def __init__(self, intent_list, entity_list, file_folder):\n",
    "        self.file_folder = file_folder\n",
    "        self.intent_list = intent_list\n",
    "        self.entity_list = entity_list\n",
    "        \n",
    "        self.bio_entity_list = self.create_BIO_tagging(self.entity_list)\n",
    "        self.number_of_entity = len(self.bio_entity_list)\n",
    "        self.number_of_intent = len(self.intent_list)\n",
    "        self.intent2id, self.id2intent = self.create_intent_dict()\n",
    "        self.tag2id, self.id2tag = self.create_entity_id_dict(self.bio_entity_list)\n",
    "        \n",
    "        self.read_json_file()\n",
    "        self.processing_data()\n",
    "        self.create_data()\n",
    "    \n",
    "    def create_intent_dict(self):\n",
    "        return {i: self.intent_list.index(i) for i in self.intent_list}, {self.intent_list.index(i)+1: i for i in self.intent_list}\n",
    "        \n",
    "    def create_BIO_tagging(self, entity_list):\n",
    "        return ['O'] + np.array([['B-'+e.upper(), 'I-'+e.upper()] for e in entity_list]).ravel().tolist()\n",
    "    \n",
    "    def create_entity_id_dict(self, bio_entity_list):\n",
    "        return {e: bio_entity_list.index(e)+1 for e in bio_entity_list}, {bio_entity_list.index(e)+1: e for e in bio_entity_list}\n",
    "    \n",
    "    def read_json_file(self):\n",
    "        self.raw_data_json = list()\n",
    "        for filename in os.listdir(self.file_folder):\n",
    "            with open(self.file_folder+'/'+filename, 'r', encoding='utf8') as f:\n",
    "                self.raw_data_json.append(json.load(f))\n",
    "                \n",
    "    def find_char_index(self, char, word, text):\n",
    "        number_of_word = text.count(word)\n",
    "        while True:\n",
    "            start_index = 0\n",
    "            char_index = text.find(char, start_index)\n",
    "            if char_index in range(text.find(word)+len(word)):\n",
    "                return char_index\n",
    "            start_index = char_index\n",
    "    \n",
    "    def check_punctuation(self, text):\n",
    "        text = text.replace('“', '\"')\n",
    "        text = text.replace('”', '\"')\n",
    "        word_list = text.split()\n",
    "        result = list()\n",
    "        punctuation_index = list()\n",
    "        isBreak = False\n",
    "        for word in word_list:\n",
    "            if len(word) == 1:\n",
    "                result+=[word]\n",
    "                continue\n",
    "            for char in word:\n",
    "                if char in set(string.punctuation):\n",
    "                    for new_word in re.split('(\\W)', word):\n",
    "                        if new_word != \"\":\n",
    "                            result+=[new_word]\n",
    "                    punctuation_index.append(self.find_char_index(char, word, text))\n",
    "                    isBreak = True\n",
    "                    break\n",
    "            if not isBreak:\n",
    "                result+=[word]\n",
    "            isBreak = False\n",
    "        return ' '.join(result), punctuation_index\n",
    "    \n",
    "    def processing_data(self):\n",
    "        self.pre_ner_processing_data = list()\n",
    "        for raw_data in self.raw_data_json:\n",
    "            for row in raw_data['examples']:\n",
    "                if len(row['seen_by']) == 0:\n",
    "                    continue\n",
    "                data = dict()\n",
    "                data['text'], punctuation_index = self.check_punctuation(row['content'])\n",
    "                data['intent_text'] = row['metadata']['Intent']\n",
    "                data['ner_token_label'] = list()\n",
    "                for annotation in row['annotations']:\n",
    "                    if annotation['annotated_by'][0]['annotator'] is None:\n",
    "                        continue\n",
    "                    start_count = 0\n",
    "                    end_count = 0\n",
    "                    for punctuation in punctuation_index:\n",
    "                        if annotation['start'] > punctuation:\n",
    "                            start_count += 1\n",
    "                        if annotation['end'] > punctuation:\n",
    "                            end_count += 1\n",
    "                    ner_token_label = dict()\n",
    "                    ner_token_label['start'] = annotation['start'] + start_count\n",
    "                    ner_token_label['end'] = annotation['end'] + end_count\n",
    "                    ner_token_label['tag'] = annotation['tag']\n",
    "                    ner_token_label['value'] = annotation['value']\n",
    "                    data['ner_token_label'].append(ner_token_label)\n",
    "                self.pre_ner_processing_data.append(data)\n",
    "                \n",
    "    def char_to_word_tagging(self, text, token_level_label):\n",
    "        word_tag_list = list()\n",
    "        isFirst = [True for i in range(len(token_level_label))]\n",
    "        isStart = False\n",
    "        for index, char in enumerate(text):\n",
    "            tag = None\n",
    "            if char == \" \":\n",
    "                word_tag_list.append(tag)\n",
    "                isStart = False\n",
    "                continue\n",
    "            if isStart:\n",
    "                continue\n",
    "            isStart = True\n",
    "            text_tag = 'O'\n",
    "            for anno_index, annotation in enumerate(token_level_label):\n",
    "                if index in range(annotation['start'], annotation['end']):\n",
    "                    if isFirst[anno_index]:\n",
    "                        isFirst[anno_index] = False\n",
    "                        text_tag = 'B-' + annotation['tag'].upper()\n",
    "                    else:\n",
    "                        text_tag = 'I-' + annotation['tag'].upper()\n",
    "            if text_tag not in self.tag2id.keys():\n",
    "                text_tag = 'O'\n",
    "            tag = self.tag2id[text_tag] - 1\n",
    "            word_tag_list.append(tag)\n",
    "        return [v for v in word_tag_list if v is not None]\n",
    "    \n",
    "    def create_data(self):\n",
    "        text = [data['text'] for data in self.pre_ner_processing_data]\n",
    "        intent_text = [data['intent_text'] for data in self.pre_ner_processing_data]\n",
    "        intent = [self.intent2id[data['intent_text']] for data in self.pre_ner_processing_data]\n",
    "        label = [self.char_to_word_tagging(data['text'], data['ner_token_label']) for data in self.pre_ner_processing_data]\n",
    "        \n",
    "        return text, intent_text, intent, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Entity_Labeler:\n",
    "    def __init__(self):\n",
    "        self.annotator = VnCoreNLP(\"VnCoreNLP-1.2.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m')\n",
    "        self.tokenizer = PhobertTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
    "    \n",
    "    def batch_process_label(self, texts, labels):\n",
    "        batch_size = len(texts)\n",
    "        tokenized_label = np.array(list(map(self.process_label, zip(texts, labels))))[:, :]\n",
    "        ner_mask = tokenized_label != 0\n",
    "        return tokenized_label, ner_mask\n",
    "    \n",
    "    def process_label(self, args):\n",
    "        text, label = args[0], args[1]\n",
    "        word_segmented_text = self.preprocessing([text])[0]\n",
    "        segmented_label = self.align_label_to_segmented_text(text, word_segmented_text, label)\n",
    "        tokens = self.tokenize([word_segmented_text])[0]\n",
    "        tokenized_label = self.align_label_to_tokenized_text(word_segmented_text, tokens, segmented_label)\n",
    "        return tokenized_label\n",
    "    \n",
    "    def batch_clean(self, batch_input):\n",
    "        def clean_text(text):\n",
    "            # text = text.lower()\n",
    "            text = \"\".join([i for i in text if i not in string.punctuation])\n",
    "            return text\n",
    "        return list(map(clean_text, batch_input))\n",
    "        \n",
    "    def preprocessing(self, input):\n",
    "        def annotate(annotator):\n",
    "            def apply(x):\n",
    "                return ' '.join([' '.join(text) for text in annotator.tokenize(x)])\n",
    "            return apply\n",
    "        return list(map(annotate(self.annotator), input))\n",
    "    \n",
    "    def process_text(self, x):\n",
    "        x = self.batch_clean(x)\n",
    "        x = self.preprocessing(x)\n",
    "        x = self.tokenizer(x, padding ='max_length',return_tensors = 'np')\n",
    "        return x['input_ids'], x['attention_mask']\n",
    "    \n",
    "    def align_label_to_segmented_text(self, text, word_segmented_text, label):\n",
    "        # Compare text and word_segmented_text to update label\n",
    "        new_label = list()\n",
    "        index = 0\n",
    "        for word in word_segmented_text.split():\n",
    "            new_label.append(label[index])\n",
    "            index += 1 + word.count('_')\n",
    "        return new_label\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        tokenized_text = self.tokenizer(text, padding ='max_length',return_tensors = 'tf')\n",
    "        reshaped_id = tf.reshape(tokenized_text['input_ids'], (tokenized_text['input_ids'].shape[0], 256, 1))\n",
    "        tokens = [[self.tokenizer.decode(x) for x in id] for id in reshaped_id]\n",
    "        return tokens\n",
    "    \n",
    "    def align_label_to_tokenized_text(self, word_segmented_text, tokens, label):\n",
    "        new_label = list()\n",
    "        previous_index = 0\n",
    "        isFirst = True\n",
    "        index = 0\n",
    "        word = word_segmented_text.split()\n",
    "\n",
    "        for token in tokens:\n",
    "            if token in ['<s>', '</s>', '<pad>']:\n",
    "                new_label.append(0)\n",
    "                isFirst = True\n",
    "            elif token == word[index]:\n",
    "                new_label.append(label[index]+1)\n",
    "                index += 1\n",
    "                isFirst = True\n",
    "            elif '@@' in token:\n",
    "                new_label.append(label[index]+1)\n",
    "                isFirst = False\n",
    "            else:\n",
    "                new_label.append(label[index]+1)\n",
    "                if index != len(label)-1:\n",
    "                    index += 1\n",
    "\n",
    "        return new_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NER_Data:\n",
    "    def __init__(self, intent_list, entity_list, data_folder):\n",
    "        self.data_loader = Data_Loader(intent_list, entity_list, data_folder)\n",
    "        self.entity_labeler = Entity_Labeler()\n",
    "\n",
    "    def create_data(self):\n",
    "        self.text, _, _, self.label = self.data_loader.create_data()\n",
    "        self.ner_label, self.ner_mask = self.entity_labeler.batch_process_label(self.text, self.label)\n",
    "        self.tokenized_text = self.entity_labeler.process_text(self.text)\n",
    "        self.input_ids = self.tokenized_text[0]\n",
    "        self.attention_mask = self.tokenized_text[1]\n",
    "        # self.tokenized_text[self.tokenized_text<3] = 0\n",
    "        # return self.tokenized_text[:, 1:], self.ner_label[:, 1:]\n",
    "        return self.input_ids, self.attention_mask, self.ner_label[:, 1:], self.ner_mask[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from dagshub storage\n",
    "\n",
    "!dvc get https://dagshub.com/nnngoc/bkchatbot entity-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTENT_LIST = [\n",
    "    'policy',\n",
    "    'answer',\n",
    "    'greeting',\n",
    "    'confirm',\n",
    "    'disagree',\n",
    "    'chitchat',\n",
    "    'thanks',\n",
    "    'faq'\n",
    "]\n",
    "ENTITY_LIST = [\n",
    "    'policy',\n",
    "    'role',\n",
    "]\n",
    "# DATA_FOLDER = '/kaggle/input/entity-data'\n",
    "DATA_PATH = '/kaggle/working/entity-data'\n",
    "TEST_SIZE = 0.1\n",
    "VALIDATION_SIZE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_creator = NER_Data(INTENT_LIST, ENTITY_LIST, DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "input_ids, attention_mask, tag, ner_mask = data_creator.create_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langauge_model = TFAutoModel.from_pretrained(\"vinai/phobert-base\").roberta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder:\n",
    "    def __init__(self, langauge_model, input_ids, attention_mask):\n",
    "        self.langauge_model = langauge_model\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.index = np.array([i for i in range(self.input_ids.shape[0])])\n",
    "        \n",
    "    def emed_text(self, batch_size=64):\n",
    "        dataset_length = self.input_ids.shape[0]\n",
    "        number_of_batch = dataset_length // batch_size\n",
    "        for i in tqdm(range(number_of_batch)):\n",
    "            index = self.index[i * batch_size : (i + 1) * batch_size]\n",
    "            input_ids = self.input_ids[index]\n",
    "            attention_mask = self.attention_mask[index]\n",
    "            \n",
    "            data=self.langauge_model(input_ids = input_ids, attention_mask = attention_mask)[0]\n",
    "\n",
    "            if i == 0:\n",
    "                self.data= data\n",
    "            else:\n",
    "                self.data=tf.concat([self.data, data], 0)\n",
    "        \n",
    "        if number_of_batch*batch_size < dataset_length:\n",
    "            index = self.index[number_of_batch * batch_size:]\n",
    "            input_ids = self.input_ids[index]\n",
    "            attention_mask = self.attention_mask[index]\n",
    "            \n",
    "            data=self.langauge_model(input_ids = input_ids, attention_mask = attention_mask)[0]\n",
    "            \n",
    "            self.data=tf.concat([self.data, data], 0)\n",
    "        \n",
    "        self.data = self.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = Embedder(langauge_model, input_ids, attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder.emed_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'cased_with_lm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'entity_data_{name}.npy'.format(name), embedder.data)\n",
    "np.save(f'entity_tag_{name}.npy'.format(name), tag)\n",
    "np.save(f'entity_mask_{name}.npy'.format(name), ner_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(f'entity_data_{name}.npy'.format(name))\n",
    "tag = np.load(f'entity_tag_{name}.npy'.format(name))\n",
    "mask = np.load(f'entity_mask_{name}.npy'.format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = [i for i in range(data.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index, test_text, train_tag, test_tag = train_test_split(index, tag, test_size=TEST_SIZE)\n",
    "train_index, validation_index, train_tag, validation_tag = train_test_split(train_index, train_tag, test_size=VALIDATION_SIZE)\n",
    "\n",
    "train_data = data[train_index]\n",
    "validation_data = data[validation_index]\n",
    "test_data = data[test_text]\n",
    "\n",
    "train_mask = mask[train_index]\n",
    "validation_mask = mask[validation_index]\n",
    "test_mask = mask[test_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.convert_to_tensor(train_data)\n",
    "validation_data = tf.convert_to_tensor(validation_data)\n",
    "test_data = tf.convert_to_tensor(test_data)\n",
    "\n",
    "train_mask = tf.convert_to_tensor(train_mask)\n",
    "validation_mask = tf.convert_to_tensor(validation_mask)\n",
    "test_mask = tf.convert_to_tensor(test_mask)\n",
    "\n",
    "train_tag = tf.convert_to_tensor(train_tag)\n",
    "validation_tag = tf.convert_to_tensor(validation_tag)\n",
    "test_tag = tf.convert_to_tensor(test_tag)\n",
    "\n",
    "entity_train_dataset = tf.data.Dataset.from_tensor_slices(({'data': train_data, 'mask': train_mask}, train_tag)).batch(BATCH_SIZE)\n",
    "entity_validation_dataset = tf.data.Dataset.from_tensor_slices(({'data': validation_data, 'mask': validation_mask}, validation_tag)).batch(BATCH_SIZE)\n",
    "entity_test_dataset = tf.data.Dataset.from_tensor_slices(({'data': test_data, 'mask': test_mask}, test_tag)).batch(BATCH_SIZE)\n",
    "\n",
    "entity_train_dataset.cache()\n",
    "entity_validation_dataset.cache()\n",
    "entity_test_dataset.cache()\n",
    "\n",
    "entity_train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "entity_validation_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "entity_test_dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autolog MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.keras.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PHOBERT_BLSTM_CRF(tf.keras.Model):\n",
    "    def __init__(self, blstm_units, dropout, num_labels):\n",
    "        super().__init__()\n",
    "        self.emb = tf.keras.layers.Bidirectional(\n",
    "                    tf.keras.layers.LSTM(blstm_units,\n",
    "                         return_sequences=True,\n",
    "                         recurrent_dropout=dropout\n",
    "                    ))\n",
    "        self.dense = tf.keras.layers.TimeDistributed(\n",
    "                        tf.keras.layers.Dense(\n",
    "                            num_labels,\n",
    "                            activation=\"relu\",\n",
    "                            kernel_regularizer=tf.keras.regularizers.L2(0.00001),\n",
    "                            bias_regularizer=tf.keras.regularizers.L2(0.00001),\n",
    "                            activity_regularizer=tf.keras.regularizers.L2(0.00001),\n",
    "                    ))\n",
    "        self.crf = CRF(num_labels)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.emb(x['data'])\n",
    "        out = self.dense(out)\n",
    "        out = self.crf(inputs=out[:, 1:, :], mask=x['mask'])\n",
    "        return out\n",
    "    \n",
    "    def compute_loss(self, x, y, sample_weight, training=False):\n",
    "        y_pred = self(x, training=training)\n",
    "        _, potentials, sequence_length, chain_kernel = y_pred\n",
    "\n",
    "        # we now add the CRF loss:\n",
    "        crf_loss = -crf_log_likelihood(potentials, y, sequence_length, chain_kernel)[0]\n",
    "\n",
    "        if sample_weight is not None:\n",
    "            crf_loss = crf_loss * sample_weight\n",
    "\n",
    "        return tf.reduce_mean(crf_loss), sum(self.losses)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x, y, sample_weight = self.unpack_data(data)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            crf_loss, internal_losses = self.compute_loss(\n",
    "                x, y, sample_weight, training=True\n",
    "            )\n",
    "            total_loss = crf_loss + internal_losses\n",
    "\n",
    "        gradients = tape.gradient(total_loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "        return {\"loss\": total_loss}\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        x, y, sample_weight = self.unpack_data(data)\n",
    "        \n",
    "        crf_loss, internal_losses = self.compute_loss(x, y, sample_weight)\n",
    "        total_loss = crf_loss + internal_losses\n",
    "        \n",
    "        return {\"loss\": total_loss}\n",
    "        \n",
    "    def unpack_data(self, data):\n",
    "        if len(data) == 2:\n",
    "            return data[0], data[1], None\n",
    "        elif len(data) == 3:\n",
    "            return data\n",
    "        else:\n",
    "            raise TypeError(\"Expected data to be a tuple of size 2 or 3.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BILSTM_NUM_UNITS = 256\n",
    "DROP_OUT = 0.1\n",
    "NUM_LABELS = data_creator.data_loader.number_of_entity + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PHOBERT_BLSTM_CRF(BILSTM_NUM_UNITS, DROP_OUT, NUM_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.001\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new\n",
    "optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x=entity_train_dataset,\n",
    "    epochs=15,\n",
    "    validation_data=entity_validation_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['loss', 'val_loss'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_tag.numpy().ravel()\n",
    "y_pred = model.predict({'data': test_data, 'mask': test_mask})[0].ravel()\n",
    "index = np.where((y_true!=0)&(y_true!=1))\n",
    "y_true = y_true[index]\n",
    "y_pred = y_pred[index]\n",
    "labels = [2, 3, 4, 5]\n",
    "target_names = data_creator.data_loader.bio_entity_list[1:]\n",
    "print(classification_report(y_true, y_pred, labels=labels, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = tag.ravel()\n",
    "y_pred = model.predict({'data': data, 'mask': mask})[0].ravel()\n",
    "index = np.where((y_true!=0)&(y_true!=1))\n",
    "y_true = y_true[index]\n",
    "y_pred = y_pred[index]\n",
    "labels = [2, 3, 4, 5]\n",
    "target_names = data_creator.data_loader.bio_entity_list[1:]\n",
    "print(classification_report(y_true, y_pred, labels=labels, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Saving and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f'trained_models/entity_model_{name}'.format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'cased_with_lm'\n",
    "filename = f'trained_models/entity_model_{name}'.format(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.make_archive('ner_model', 'zip', filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /kaggle/working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink \n",
    "FileLink(r'ner_model.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Registering the model in the registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model =  keras.models.load_model(\"/kaggle/working/trained_models/entity_model_cased_with_lm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.keras.save_model(keras_model, \"mlflow/model\", custom_objects={'Addons>F1Score': tfa.metrics.F1Score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requirements = \"\"\"\n",
    "mlflow==2.5.0\n",
    "tensorflow==2.12.0\n",
    "cloudpickle==2.2.1\n",
    "tensorflow_addons\n",
    "\"\"\"\n",
    "\n",
    "f = open(\"/kaggle/working/mlflow/model/requirements.txt\", \"w\")\n",
    "f.write(requirements)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "conda = {\n",
    "    'name': 'mlflow-env',\n",
    "    'channels': ['conda-forge'],\n",
    "    'dependencies': [\n",
    "        'python=3.10.12',\n",
    "        'pip',\n",
    "        {\n",
    "            'pip': [\n",
    "                'mlflow==2.5.0',\n",
    "                'tensorflow==2.12.0',\n",
    "                'tensorflow_addons',\n",
    "                'cloudpickle==2.2.1',\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "file=open(\"/kaggle/working/mlflow/model/conda.yaml\",\"w\")\n",
    "yaml.dump(conda, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"entity-classifier\"\n",
    "model_local_path = \"/kaggle/working/mlflow/model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_client = MlflowClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registered_model = mlflow.register_model(f\"file://{model_local_path}\", model_name)\n",
    "version = registered_model.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"We are going to deploy model {model_name} with version {version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = \"entity-classifier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_client = get_deploy_client(mlflow.get_tracking_uri())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = deployment_client.create_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_uri = deployment_client.get_endpoint(endpoint=endpoint_name)[\"properties\"][\n",
    "    \"scoringUri\"\n",
    "]\n",
    "print(scoring_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_name = \"default\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_config_path = \"deployment_config.json\"\n",
    "with open(deployment_config_path, \"w\") as outfile:\n",
    "    outfile.write(json.dumps(deploy_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment = deployment_client.create_deployment(\n",
    "    name=deployment_name,\n",
    "    endpoint=endpoint_name,\n",
    "    model_uri=f\"models:/{model_name}/{version}\",\n",
    "    config={\"deploy-config-file\": deployment_config_path},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assign all the traffic to the created deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_config = {\"traffic\": {deployment_name: 100}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_config_path = \"traffic_config.json\"\n",
    "with open(traffic_config_path, \"w\") as outfile:\n",
    "    outfile.write(json.dumps(traffic_config))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
